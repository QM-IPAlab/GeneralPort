<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="human-robot interaction, handover, hand-object reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DiffPort: Adapting Pre-trained Diffusion Models for Generalizable Robot Manipulation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DiffPort: Adapting Pre-trained Diffusion Models for Generalizable
              Robot Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=12U1mcgAAAAJ" target="_blank">Chaoran
                  Zhu</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://kerolex.github.io/" target="_blank">Junyoung Seo</a><sup>*2</sup>,</span>
              <span class="author-block">
                Aliyasin El Ayouch<sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://emmanuel-senft.github.io/" target="_blank">Emmanuel Senft</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=cIK1hS8AAAAJ&hl=ko" target="_blank">Seungryong
                  Kim</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://eecs.qmul.ac.uk/~coh/" target="_blank">Changjae Oh</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Queen Mary University of London, UK<sup>1</sup><br>
                Korea Advanced Institute of Science & Technology, Korea<sup>2</sup>
                <br>Idiap Research Institute, Switzerland<sup>3</sup></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (Under review)
                    </span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/QM-IPAlab/StereoHO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%" width="100%">
          <source src="static/videos/teaser video.mp4" type="video/mp4">
        </video>

        <h2 class="subtitle has-text-centered">
          We present DiffPort, a visual-languae model which leverage pre-trained diffsuion models for robot maniplation
          tasks. Our model applies to a wide range of robot tasks in both simulation and real world.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We introduce a new framework that leverages a pre-trained text-to-image diffusion model for
              language-guided robot manipulation. We use a learnable captioner that transforms the textual commands for
              robot action into text embeddings aligned with the pre-trained diffusion model. We then utilize the
              diffusion-aligned text embeddings and the visual observations as input to extract features from the
              diffusion model. These semantic features are then integrated with an affordance prediction network that
              guides robot actions for pick and place tasks. We validate our framework on diverse language-guided
              tabletop robot manipulation tasks in both simulation and real-world environments. The results demonstrate
              the advantages of our approach in manipulating previously seen and unseen objects.
            </p>
          </div>
          <div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">Pipeline</h2>
            <div class="level-set has-text-justified">
              <p>
                Our model processes an RGB-D observation and corresponding robot commands during manipulation. In the
                decoder, features from the diffusion model and the transporter are fused through concatenation, while
                features from the text encoder are integrated via element-wise multiplication.
              </p>
            </div>
          </div>
          <div class="columns is-centered">
            <img src="static/images/pipeline.jpg" alt="Robot setup" width="100%" class="center-image" />
          </div>
        </div>
      </div>
  </section>

  <section class="hero section is-small is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">Robot Setup</h2>
            <div class="level-set has-text-justified">
              <p>
                We conduct experiments using a Franka Emika Panda robot equipped with an RGB-D end-effector camera. We
                also show our seen and unseen split of objects and colored blocks in the manipulation tasks.
              </p>
            </div>
          </div>
          <div class="columns is-centered">
            <img src="static/images/robot_setup.jpg" alt="Reconstruction pipeline" width="100%" class="center-image" />
          </div>
        </div>
      </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h2 class="title is-3">Real robot experiments</h2>
        <div class="level-set has-text-justified">
          <div class="container has-text-centered subtitle">
            Seen colors and objects
          </div>
        </div>
        <div class="video-container">
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-seen-01.mp4" type="video/mp4">
            </video>
            <div class="description">Put the yellow block in the red bowl</div>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-seen-02.mp4" type="video/mp4">
            </video>
            <div class="description">Put the blue block in the red bowl</div>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-seen-03.mp4" type="video/mp4">
            </video>
            <div class="description">Put the screwdriver in the brown box</div>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-seen-04.mp4" type="video/mp4">
            </video>
            <div class="description">Put the banana in the brown box</div>
          </div>
        </div>
        <div class="level-set has-text-justified">
          <div class="container has-text-centered subtitle">
            <br> 
            Unseen colors and objects
          </div>
        </div>
        <div class="video-container">
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-unseen-01.mp4" type="video/mp4">
            </video>
            <div class="description">Put the orange block in the red bowl</div>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-unseen-02.mp4" type="video/mp4">
            </video>
            <div class="description">Put the green block in the red bowl</div>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-unseen-03.mp4" type="video/mp4">
            </video>
            <div class="description">Put the strawberry in the brown box</div>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/real-unseen-04.mp4" type="video/mp4">
            </video>
            <div class="description">Put the golf ball in the brown box</div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h2 class="title is-3">Simulation experiments</h2>
        <div class="level-set has-text-justified">
          <div class="container has-text-centered subtitle">
            Seen colors and objects
          </div>
        </div>
        <div class="video-container">
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-seen-01.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-seen-02.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-seen-03.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-seen-04.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="level-set has-text-justified">
          <div class="container has-text-centered subtitle">
            <br> 
            Unseen colors and objects
          </div>
        </div>
        <div class="video-container">
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-unseen-01.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-unseen-02.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-unseen-03.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-item">
            <video class="comp_video" poster="" id="tree" autoplay controls muted loop>
              <source src="static/videos/sim-unseen-04.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero section is-small ">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">Affordance Prediction</h2>
            <div class="level-set has-text-justified">
              <p>
                Affordances predictions for both pick and place. DiffPort is able to generate affordance predictions and
                localize objects without using any explicit object representations (e.g., object detections and
                segmentation)
              </p>
            </div>
          </div>
          <div class="columns is-centered">
            <img src="static/images/affordance.jpg" alt="Reconstruction pipeline" width="100%" class="center-image" />
          </div>
        </div>
      </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>